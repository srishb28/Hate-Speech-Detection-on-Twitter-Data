{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import precision_recall_fscore_support\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__dataset is split into train and test, with 11000 tweets in training set and the rest in test dataset__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df=pd.read_csv(\"final_dataset.csv\")\n",
    "test_data = df[11000:].copy()\n",
    "data = df[:11000].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_processing(raw_tweet):\n",
    "    letters_only=re.sub(\"[^a-zA-Z]\",\" \",raw_tweet)\n",
    "    words=letters_only.lower().split()\n",
    "    stops=set(stopwords.words(\"english\"))\n",
    "    m_w=[w for w in words if not w in stops]\n",
    "    return (\" \".join(m_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tweets=data[\"Tweets\"].size\n",
    "clean_tweet=[]\n",
    "for i in range(0,num_tweets):\n",
    "    clean_tweet.append(tweet_processing(data[\"Tweets\"][i]))\n",
    "data[\"Tweets\"]=clean_tweet \n",
    "\n",
    "\n",
    "num_tweets_test=test_data[\"Tweets\"].size\n",
    "clean_tweet_test=[]\n",
    "for i in range(num_tweets,num_tweets+num_tweets_test):\n",
    "    clean_tweet_test.append(tweet_processing(test_data[\"Tweets\"][i]))\n",
    "test_data[\"Tweets\"]=clean_tweet_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model : SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test_svm, Y_train, Y_test_svm = train_test_split(df.Tweets, df.Class, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer=CountVectorizer(analyzer = \"word\",tokenizer = None,preprocessor = None,stop_words = None,max_features = 5000)\n",
    "\n",
    "train_data_features=vectorizer.fit_transform(X_train)\n",
    "train_data_features=train_data_features.toarray()\n",
    "\n",
    "test_data_features=vectorizer.transform(X_test_svm)\n",
    "test_data_features=test_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Testing\n",
      "Accuracy:  0.8618101545253863\n"
     ]
    }
   ],
   "source": [
    "#SVM with linear kernel\n",
    "clf=svm.SVC(kernel='linear',C=1.0)\n",
    "print (\"Training\")\n",
    "clf.fit(train_data_features,Y_train)\n",
    "\n",
    "print (\"Testing\")\n",
    "predicted=clf.predict(test_data_features)\n",
    "accuracy=np.mean(predicted==Y_test_svm)\n",
    "print (\"Accuracy: \",accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__precision,recall,F1 score for svm model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6150747931093572, 0.6313465783664459, 0.622672398462044, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Srishti\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "score_svm=precision_recall_fscore_support(Y_test, predicted, average='weighted')\n",
    "print(score_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__creating one hot vector for classes(labels).__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sexism']=0\n",
    "data['racism']=0\n",
    "data['none']=0\n",
    "\n",
    "data['sexism'] = np.where(data['Class'] == 'sexism', 1, 0)\n",
    "data['racism'] = np.where(data['Class'] == 'racism', 1, 0)\n",
    "data['none'] = np.where(data['Class'] == 'none', 1, 0)\n",
    "#data.head()\n",
    "columns=['sexism','racism','none']\n",
    "y=data[columns].values\n",
    "#print(y.shape)\n",
    "\n",
    "\n",
    "test_data['sexism']=0\n",
    "test_data['racism']=0\n",
    "test_data['none']=0\n",
    "\n",
    "test_data['sexism'] = np.where(test_data['Class'] == 'sexism', 1, 0)\n",
    "test_data['racism'] = np.where(test_data['Class'] == 'racism', 1, 0)\n",
    "test_data['none'] = np.where(test_data['Class'] == 'none', 1, 0)\n",
    "#data.head()\n",
    "columns=['sexism','racism','none']\n",
    "y_test=test_data[columns].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Tokenizing words__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "#text1 = \"It's true that the chicken was the best bamboozler in the known multiverse.\"\n",
    "#tokens = word_tokenize(data['Tweets'])\n",
    "data['tokenized_sents'] = data.apply(lambda column: word_tokenize(column['Tweets']), axis=1)\n",
    "test_data['tokenized_sents'] = test_data.apply(lambda column: word_tokenize(column['Tweets']), axis=1)\n",
    "df['tokenized_sents'] = df.apply(lambda column: word_tokenize(column['Tweets']), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Creating word embeddings__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# define training data\n",
    "sentences = df['tokenized_sents']\n",
    "# train model\n",
    "model = Word2Vec(sentences,size=200,window =4,min_count=1,sg=1)\n",
    "#print(model)\n",
    "words = list(model.wv.vocab)\n",
    "#print(len(words))\n",
    "model.save('model.bin')\n",
    "\n",
    "#model = Word2Vec.load('model.bin')\n",
    "#print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__creating embedding matrix__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Srishti\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((vocab_size,200))\n",
    "\n",
    "for i in range(0,len(words)):\n",
    "    embedding_vector = model[words[i]]\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "embedding_matrix[vocab_size-1]= np.random.normal(scale=0.6, size=(200, ))\n",
    "    \n",
    "#print(embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Using Keras to train LSTM model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__resizing each tweet to size 50__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data['tokenized_sents'])\n",
    "sequences = tokenizer.texts_to_sequences(data['tokenized_sents'])\n",
    "X_t = pad_sequences(sequences, maxlen=50)\n",
    "vocab_size = 22194\n",
    "#print(vocab_size)\n",
    "tokenizer.fit_on_texts(test_data['tokenized_sents'])\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data['tokenized_sents'])\n",
    "X_test = pad_sequences(test_sequences, maxlen=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__calculating a user's tendency towards racism, sexism, neutrality by taking the ratio of number of tweets marked as a particular label and total number of tweets of that user. This is done for each label(sexism, racism and neutral)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[949380854 297877558 272704749 ... 289846547  29424561 289846547]\n"
     ]
    }
   ],
   "source": [
    "count1=[]\n",
    "print(data['User Id'].values)\n",
    "for i in data['User Id'].unique():\n",
    "    #print(i)\n",
    "    count1.append((data['User Id']== i).sum())\n",
    "#print(count1)\n",
    "neutral_count=[]\n",
    "sexism_count=[]\n",
    "racism_count=[]\n",
    "for i in data['User Id'].unique():\n",
    "    neutral_count.append(len(data[(data['User Id']==i) & (data['Class']=='none')]))\n",
    "    sexism_count.append(len(data[(data['User Id']==i) & (data['Class']=='sexism')]))\n",
    "    racism_count.append(len(data[(data['User Id']==i) & (data['Class']=='racism')])) \n",
    "    \n",
    "count_test=[]\n",
    "#print(data['User Id'].values)\n",
    "for i in test_data['User Id'].unique():\n",
    "    #print(i)\n",
    "    count_test.append((test_data['User Id']== i).sum())\n",
    "#print(count1)\n",
    "neutral_count_test=[]\n",
    "sexism_count_test=[]\n",
    "racism_count_test=[]\n",
    "for i in test_data['User Id'].unique():\n",
    "    #neutral_count.append((data['User Id']==i) & (data['Class']== 'none').sum())\n",
    "    neutral_count_test.append(len(test_data[(test_data['User Id']==i) & (test_data['Class']=='none')]))\n",
    "    sexism_count_test.append(len(test_data[(test_data['User Id']==i) & (test_data['Class']=='sexism')]))\n",
    "    racism_count_test.append(len(test_data[(test_data['User Id']==i) & (test_data['Class']=='racism')])) \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_len = len(data['User Id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_sexism=[]\n",
    "ratio_neutral=[]\n",
    "ratio_racism=[]\n",
    "\n",
    "for i in range(0,X_len):\n",
    "    ratio_sexism.append(sexism_count[i]/count1[i])\n",
    "    ratio_racism.append(racism_count[i]/count1[i])\n",
    "    ratio_neutral.append(neutral_count[i]/count1[i])\n",
    "        \n",
    "#make a column for ratio of eachc class\n",
    "\n",
    "ratio_sexism_test=[]\n",
    "ratio_neutral_test=[]\n",
    "ratio_racism_test=[]\n",
    "\n",
    "for i in range(0,len(test_data['User Id'].unique())):\n",
    "    ratio_sexism_test.append(sexism_count_test[i]/count_test[i])\n",
    "    ratio_racism_test.append(racism_count_test[i]/count_test[i])\n",
    "    ratio_neutral_test.append(neutral_count_test[i]/count_test[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "j=0\n",
    "for i in data['User Id'].unique():\n",
    "    \n",
    "    #print(i)\n",
    "    data.loc[data['User Id'] == i,'tendency_sexual'] = ratio_sexism[j]\n",
    "    data.loc[data['User Id'] == i,'tendency_racism'] = ratio_racism[j]\n",
    "    data.loc[data['User Id'] == i,'tendency_neutral'] = ratio_neutral[j]\n",
    "    j=j+1\n",
    "\n",
    "k=0\n",
    "for i in test_data['User Id'].unique():\n",
    "    \n",
    "    #print(i)\n",
    "    test_data.loc[test_data['User Id'] == i,'tendency_sexual'] = ratio_sexism_test[k]\n",
    "    test_data.loc[test_data['User Id'] == i,'tendency_racism'] = ratio_racism_test[k]\n",
    "    test_data.loc[test_data['User Id'] == i,'tendency_neutral'] = ratio_neutral_test[k]\n",
    "    k=k+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11000, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Srishti\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\Srishti\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n",
      "C:\\Users\\Srishti\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\Srishti\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\Srishti\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\Srishti\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "tendency_sexism=data['tendency_sexual'].as_matrix()\n",
    "tendency_racism=data['tendency_racism'].as_matrix()\n",
    "tendency_neutral=data['tendency_neutral'].as_matrix()\n",
    "tendency_sexism=tendency_sexism.reshape(len(tendency_sexism),1)\n",
    "tendency_racism=tendency_sexism.reshape(len(tendency_racism),1)\n",
    "tendency_neutral=tendency_sexism.reshape(len(tendency_neutral),1)\n",
    "print(tendency_sexism.shape)\n",
    "\n",
    "tendency_sexism_test=test_data['tendency_sexual'].as_matrix()\n",
    "tendency_racism_test=test_data['tendency_racism'].as_matrix()\n",
    "tendency_neutral_test=test_data['tendency_neutral'].as_matrix()\n",
    "tendency_sexism_test=tendency_sexism_test.reshape(len(tendency_sexism_test),1)\n",
    "tendency_racism_test=tendency_sexism_test.reshape(len(tendency_racism_test),1)\n",
    "tendency_neutral_test=tendency_sexism_test.reshape(len(tendency_neutral_test),1)\n",
    "#print(tendency_sexism.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__concatenating word vectors with tendencies of users calculated above for each label__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11000, 53)\n"
     ]
    }
   ],
   "source": [
    "#print(X_t)\n",
    "X1_t=np.concatenate((X_t, tendency_sexism), axis=1)\n",
    "X1_t=np.concatenate((X1_t, tendency_racism), axis=1)\n",
    "X1_t=np.concatenate((X1_t, tendency_neutral), axis=1)\n",
    "print(X1_t.shape)\n",
    "\n",
    "\n",
    "X1_test=np.concatenate((X_test, tendency_sexism_test), axis=1)\n",
    "X1_test=np.concatenate((X1_test, tendency_racism_test), axis=1)\n",
    "X1_test=np.concatenate((X1_test, tendency_neutral_test), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Training LSTM Model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9900 samples, validate on 1100 samples\n",
      "Epoch 1/5\n",
      "9900/9900 [==============================] - 58s 6ms/step - loss: 0.4043 - acc: 0.8264 - val_loss: 0.2793 - val_acc: 0.8912\n",
      "Epoch 2/5\n",
      "9900/9900 [==============================] - 52s 5ms/step - loss: 0.2755 - acc: 0.9053 - val_loss: 0.4068 - val_acc: 0.9142\n",
      "Epoch 3/5\n",
      "9900/9900 [==============================] - 51s 5ms/step - loss: 0.2153 - acc: 0.9056 - val_loss: 0.3078 - val_acc: 0.9115\n",
      "Epoch 4/5\n",
      "9900/9900 [==============================] - 52s 5ms/step - loss: 0.1474 - acc: 0.8856 - val_loss: 0.5323 - val_acc: 0.8652\n",
      "Epoch 5/5\n",
      "9900/9900 [==============================] - 52s 5ms/step - loss: 0.1172 - acc: 0.8606 - val_loss: 0.5688 - val_acc: 0.8091\n"
     ]
    }
   ],
   "source": [
    "inp = Input(shape=(53,))\n",
    "x = Embedding(vocab_size,200,weights=[embedding_matrix])(inp)\n",
    "#print(x.values)\n",
    "x = (LSTM(50, return_sequences=True, dropout=0.2, recurrent_dropout=0.4))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dense(50)(x)\n",
    "x=LeakyReLU(alpha=0.02)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(3)(x)\n",
    "x=LeakyReLU(alpha=0.02)(x)\n",
    "model1 = Model(inputs=inp, outputs=x)\n",
    "model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model1.fit(X1_t,y, batch_size=32, epochs=5, validation_split=0.1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving model\n",
    "model1.save('model_lstm.h5')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__training a bideirectional LSTM model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9900 samples, validate on 1100 samples\n",
      "Epoch 1/5\n",
      "9900/9900 [==============================] - 72s 7ms/step - loss: 0.4401 - acc: 0.8079 - val_loss: 0.2553 - val_acc: 0.9033\n",
      "Epoch 2/5\n",
      "9900/9900 [==============================] - 64s 6ms/step - loss: 0.3585 - acc: 0.8989 - val_loss: 0.3089 - val_acc: 0.9248\n",
      "Epoch 3/5\n",
      "9900/9900 [==============================] - 64s 7ms/step - loss: 0.2007 - acc: 0.9061 - val_loss: 0.2847 - val_acc: 0.9173\n",
      "Epoch 4/5\n",
      "9900/9900 [==============================] - 67s 7ms/step - loss: 0.4475 - acc: 0.8161 - val_loss: 0.6964 - val_acc: 0.8045\n",
      "Epoch 5/5\n",
      "9900/9900 [==============================] - 66s 7ms/step - loss: 0.1546 - acc: 0.8701 - val_loss: 0.4702 - val_acc: 0.8542\n"
     ]
    }
   ],
   "source": [
    "inp_bi = Input(shape=(53,))\n",
    "x_bi = Embedding(vocab_size,200,weights=[embedding_matrix])(inp_bi)\n",
    "#print(x.values)\n",
    "x_bi = Bidirectional(LSTM(50, return_sequences=True, dropout=0.2, recurrent_dropout=0.4))(x_bi)\n",
    "x_bi = GlobalMaxPool1D()(x_bi)\n",
    "x_bi = Dense(50)(x_bi)\n",
    "x_bi=LeakyReLU(alpha=0.02)(x_bi)\n",
    "x_bi = Dropout(0.2)(x_bi)\n",
    "x_bi = Dense(3)(x_bi)\n",
    "x_bi=LeakyReLU(alpha=0.02)(x_bi)\n",
    "model_bi = Model(inputs=inp_bi, outputs=x_bi)\n",
    "model_bi.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_bi.fit(X1_t,y, batch_size=32, epochs=5, validation_split=0.1);\n",
    "model_bi.save('model_bi.h5') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Calculating  precision,recall,F1score, of LSTM and bidirectional model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_lstm = model1.predict(X1_test)\n",
    "predicted_bi = model_bi.predict(X1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_lstm = pd.DataFrame.from_records(predicted_lstm)\n",
    "dataframe_bi=pd.DataFrame.from_records(predicted_bi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__predicted class is stored in a column of dataframe__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        none\n",
      "1        none\n",
      "2        none\n",
      "3        none\n",
      "4        none\n",
      "5      Sexism\n",
      "6        none\n",
      "7        none\n",
      "8        none\n",
      "9        none\n",
      "10       none\n",
      "11       none\n",
      "12       none\n",
      "13       none\n",
      "14       none\n",
      "15       none\n",
      "16       none\n",
      "17       none\n",
      "18       none\n",
      "19       none\n",
      "20       none\n",
      "21       none\n",
      "22       none\n",
      "23       none\n",
      "24       none\n",
      "25       none\n",
      "26       none\n",
      "27       none\n",
      "28       none\n",
      "29       none\n",
      "        ...  \n",
      "295      none\n",
      "296    Sexism\n",
      "297    Sexism\n",
      "298      none\n",
      "299      none\n",
      "300      none\n",
      "301      none\n",
      "302      none\n",
      "303      none\n",
      "304      none\n",
      "305      none\n",
      "306      none\n",
      "307      none\n",
      "308      none\n",
      "309      none\n",
      "310      none\n",
      "311      none\n",
      "312    Sexism\n",
      "313    Sexism\n",
      "314      none\n",
      "315    Sexism\n",
      "316      none\n",
      "317    Sexism\n",
      "318      none\n",
      "319      none\n",
      "320      none\n",
      "321      none\n",
      "322      none\n",
      "323      none\n",
      "324      none\n",
      "Name: predClass, Length: 325, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(dataframe_lstm['predClass'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__bidirectional lstm results__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        none\n",
      "1        none\n",
      "2        none\n",
      "3        none\n",
      "4        none\n",
      "5        none\n",
      "6        none\n",
      "7        none\n",
      "8        none\n",
      "9        none\n",
      "10       none\n",
      "11       none\n",
      "12       none\n",
      "13       none\n",
      "14       none\n",
      "15       none\n",
      "16       none\n",
      "17       none\n",
      "18       none\n",
      "19       none\n",
      "20       none\n",
      "21       none\n",
      "22       none\n",
      "23       none\n",
      "24       none\n",
      "25       none\n",
      "26       none\n",
      "27       none\n",
      "28       none\n",
      "29       none\n",
      "        ...  \n",
      "295      none\n",
      "296      none\n",
      "297      none\n",
      "298      none\n",
      "299      none\n",
      "300      none\n",
      "301      none\n",
      "302      none\n",
      "303      none\n",
      "304      none\n",
      "305      none\n",
      "306      none\n",
      "307      none\n",
      "308      none\n",
      "309      none\n",
      "310      none\n",
      "311      none\n",
      "312    Sexism\n",
      "313    Sexism\n",
      "314      none\n",
      "315    Sexism\n",
      "316      none\n",
      "317      none\n",
      "318      none\n",
      "319      none\n",
      "320      none\n",
      "321      none\n",
      "322      none\n",
      "323      none\n",
      "324      none\n",
      "Name: predClass, Length: 325, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(dataframe_bi['predClass'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__get label function creates appropriate labels according to  predictions__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_label(df):\n",
    "    if ((df[0] >df[1]) & (df[0] > df[2])):\n",
    "        return 'Sexism'\n",
    "    elif ((df[1] >df[0]) & (df[1] > df[2])):\n",
    "        return 'Racism'\n",
    "    else:\n",
    "        return 'none'\n",
    "\n",
    "\n",
    "dataframe_lstm['predClass'] = dataframe_lstm.apply(lambda row: get_label(row), axis=1)\n",
    "dataframe_bi['predClass'] = dataframe_bi.apply(lambda row: get_label(row), axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__LSTM score__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8760505166475316, 0.7723076923076924, 0.8209144701452393, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Srishti\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Srishti\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "score_lstm=precision_recall_fscore_support(test_data['Class'],dataframe_lstm['predClass'],average='weighted')\n",
    "print(score_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Bidirectional LSTM score__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8706521160500399, 0.8276923076923077, 0.8486288753405109, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Srishti\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Srishti\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "score_bi=precision_recall_fscore_support(test_data['Class'],dataframe_bi['predClass'],average='weighted')\n",
    "print(score_bi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__precision recall and f1 scores for svm and deep learning models__:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Precision    Recall  F1-score\n",
      "SVM                  0.615075  0.631347  0.622672\n",
      "LSTM                 0.876051  0.772308  0.820914\n",
      "Bidirectional LSTM   0.870652  0.827692  0.848629\n"
     ]
    }
   ],
   "source": [
    "summary = [[score_svm[0],score_svm[1],score_svm[2]], [score_lstm[0],score_lstm[1],score_lstm[2] ],[score_bi[0],score_bi[1],score_bi[2]]]\n",
    "score=pd.DataFrame(summary, columns=[\"Precision\", \"Recall\",\"F1-score\"])\n",
    "score.rename(index={0:'SVM',1:'LSTM',2:'Bidirectional LSTM'}, inplace=True)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
